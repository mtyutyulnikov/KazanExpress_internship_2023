{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import random\n",
    "import numpy as np\n",
    "from model import ClassifierNet\n",
    "import torch\n",
    "from dataset import ItemsDataModule\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kfold_4_08673.ckpt',\n",
       " 'kfold_2_08663.ckpt',\n",
       " 'kfold_3_08646.ckpt',\n",
       " 'kfold_0_08651.ckpt',\n",
       " 'kfold_1_08661.ckpt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"kfold_models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/michael/miniconda3/envs/kazan_express/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368e4ec4219941dbb7e138fbc8d05e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cecd547c114531a38c12ed232d780a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f2e50102874f35947639736f7dd794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d585ba5cfe624994bac084e424ae65c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a186a7b80eb4f0bb52698f4a0bb4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_net = ClassifierNet(\n",
    "    {\"num_classes\": 730},\n",
    ")\n",
    "\n",
    "dm = ItemsDataModule(batch_size=4, num_workers=20)\n",
    "dm.setup()\n",
    "\n",
    "\n",
    "kfold_preds = []\n",
    "for file in os.listdir(\"kfold_models\"):\n",
    "    default_net = default_net.load_from_checkpoint(f\"kfold_models/{file}\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "    )\n",
    "\n",
    "    p = trainer.predict(default_net, dm.test_dataloader())\n",
    "    kfold_preds.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = []\n",
    "probs_all = []\n",
    "\n",
    "for p in kfold_preds:\n",
    "    preds = torch.cat([i[0] for i in p])\n",
    "    probs = torch.cat([i[4] for i in p])\n",
    "    preds_all.append(preds)\n",
    "    probs_all.append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16860, 5]), torch.Size([16860, 730, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_all = torch.stack(preds_all).T\n",
    "probs_all = torch.stack(probs_all).transpose(0, 1).transpose(1, 2)\n",
    "preds_all.shape, probs_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"preprocessed_train.parquet\")\n",
    "test_df = pd.read_parquet(\"preprocessed_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, (p, probs) in enumerate(zip(preds_all, probs_all)):\n",
    "    if not (p == p[0]).all():\n",
    "        counter += 1\n",
    "counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что в целом 3к из 16к емплов неоднозначностей в результатах ансамбля. Что неплохо\n",
    "\n",
    "Посмотрим некоторые примеры "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997646\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9350, 0.5117, 0.9934, 0.4364, 0.5368]),\n",
      "indices=tensor([453, 453, 453, 428, 206]))\n",
      "Светодиодная лента Smart led Strip Light, с пультом, метров, USB, Bluetooth\n",
      "----------------------------------------------------------------------\n",
      "1668662\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8276, 0.9919, 0.9997, 0.6497, 0.9994]),\n",
      "indices=tensor([181, 438, 438, 302, 181]))\n",
      "Декоративная табличка Правила кухни подставка под горячее, разделочная доска\n",
      "----------------------------------------------------------------------\n",
      "1467778\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9154, 0.9953, 0.9976, 0.9958, 0.9218]),\n",
      "indices=tensor([242, 440, 440, 440, 440]))\n",
      "Подставка под ложку керамическая, подложка Клубника Лаванда Лимон\n",
      "----------------------------------------------------------------------\n",
      "342378\n",
      "torch.return_types.max(\n",
      "values=tensor([0.6378, 0.8902, 0.5955, 0.8247, 0.6013]),\n",
      "indices=tensor([581,  65,  65, 581, 581]))\n",
      "Топ Чебоксарский трикотаж\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (p, probs) in enumerate(zip(preds_all, probs_all)):\n",
    "    if not (p == p[0]).all():\n",
    "        print(test_df.iloc[i][\"product_id\"])\n",
    "        print(probs.max(dim=0))\n",
    "        print(test_df.iloc[i][\"title\"])\n",
    "        print(\"----------------------------------------------------------------------\")\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Разделочные подарочные доски с Новогодней тематикой',\n",
       " 'Все категории->Товары для дома->Товары для кухни->Кухонные инструменты->Разделочные доски']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 181].iloc[0][[\"title\", \"category_name\"]].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Крючок без сверления для фотографий, картин, постеров, (Крючки для твердых стен компл. )',\n",
       " 'Все категории->Товары для дома->Декор и интерьер->Таблички, номера и крючки']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 438].iloc[0][[\"title\", \"category_name\"]].tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что есть примеры товаров, где трудно самому определить тип. (Пример то ли разделочная доска, то ли табличка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_to_category_id = (\n",
    "    train_df[[\"category_id\", \"target\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"target\", drop=True)\n",
    "    .to_dict()[\"category_id\"]\n",
    ")\n",
    "len(target_to_category_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16860"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds = []\n",
    "\n",
    "for i, (p, probs) in enumerate(zip(preds_all, probs_all)):\n",
    "    if not (p == p[0]).all():\n",
    "        max_probs = probs.max(dim=0).values\n",
    "\n",
    "        counts = p.unique(return_counts=True)\n",
    "        if (counts[1] >= 3).any() or (counts[1] == 2).sum() == 1:\n",
    "            idx = counts[1].argmax()\n",
    "            pred = int(counts[0][idx])\n",
    "\n",
    "        elif (counts[1] == 2).sum() == 2:\n",
    "            top_idxs = (counts[1] == 2).nonzero(as_tuple=True)[0]\n",
    "            top_class_1 = counts[0][top_idxs[0]]\n",
    "            top_class_2 = counts[0][top_idxs[1]]\n",
    "\n",
    "            neural_nets_1 = (p == top_class_1).nonzero(as_tuple=True)[0]\n",
    "            neural_nets_2 = (p == top_class_2).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            sum_probs_1 = max_probs[neural_nets_1].sum()\n",
    "            sum_probs_2 = max_probs[neural_nets_2].sum()\n",
    "\n",
    "            pred = int(top_class_1) if sum_probs_1 > sum_probs_2 else int(top_class_2)\n",
    "        else:\n",
    "            max_prob_idx = max_probs.argmax()\n",
    "            pred = int(p[max_prob_idx])\n",
    "\n",
    "        final_preds.append(pred)\n",
    "\n",
    "    else:\n",
    "        final_preds.append(int(p[0]))\n",
    "len(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16860, 2599)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_preds = [target_to_category_id[i] for i in final_preds]\n",
    "len(category_preds), min(category_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in category_preds:\n",
    "    if i not in train_df[\"category_id\"].unique():\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"predicted_category_id\"] = category_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Чехол книжка для Honor', 11937]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[100][[\"title\", \"predicted_category_id\"]].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Все категории->Электроника->Смартфоны и телефоны->Аксессуары и запчасти->Чехлы'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"category_id\"] == 11937].iloc[0][\"category_name\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже на правду :) \n",
    "\n",
    "Удаляем лишние столбцы и коммитим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>predicted_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997646</td>\n",
       "      <td>12919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>927375</td>\n",
       "      <td>14922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1921513</td>\n",
       "      <td>2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1668662</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467778</td>\n",
       "      <td>13887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16855</th>\n",
       "      <td>1914264</td>\n",
       "      <td>11645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16856</th>\n",
       "      <td>1310569</td>\n",
       "      <td>12357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16857</th>\n",
       "      <td>978095</td>\n",
       "      <td>13651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16858</th>\n",
       "      <td>797547</td>\n",
       "      <td>2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16859</th>\n",
       "      <td>703835</td>\n",
       "      <td>11757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id  predicted_category_id\n",
       "0         1997646                  12919\n",
       "1          927375                  14922\n",
       "2         1921513                   2803\n",
       "3         1668662                  13755\n",
       "4         1467778                  13887\n",
       "...           ...                    ...\n",
       "16855     1914264                  11645\n",
       "16856     1310569                  12357\n",
       "16857      978095                  13651\n",
       "16858      797547                   2740\n",
       "16859      703835                  11757\n",
       "\n",
       "[16860 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = test_df.columns.tolist()\n",
    "columns_to_drop.remove(\"product_id\")\n",
    "columns_to_drop.remove(\"predicted_category_id\")\n",
    "result_df = test_df.drop(columns=columns_to_drop)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_parquet(\"result.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'predicted_category_id'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(\"result.parquet\").columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kazan_express",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
